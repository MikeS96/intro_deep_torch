\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[backend=biber]{biblatex}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{dsfont}
\addbibresource{bib.bib}
\setlength{\parindent}{0em}
\bibliography{bib}
\setlength{\parskip}{6pt}
\usepackage[margin=1.0in]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{url}

\title{Intro to deep learning with PyTorch}
\author{Miguel A. Saavedra-Ruiz}
\date{May 2020}
\linespread{1.0}

\nocite{*}


\begin{document}

\maketitle

\section*{Convolutional Neural Networks}

Convolutional Neural Networks (CNNs) are a variation of neural networks with the ability to efficiently operate over images. CNNs can look at images as a whole and learn to identify spatial patterns such as prominent colors and shapes, or whether a texture is fuzzy or smooth and so on. The shapes and colors that define any image and any object in an image are often called \textbf{features} Fig. \ref{fig:f1}.

An example of a feature would be what do we look at to distinguish a cat and a dog. These characteristics might be the shape of the eyes, the size, and how they move are just a couple of examples of visual features. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth,height=0.45\textheight,keepaspectratio]{images/features.png}
    \captionsetup{justification=centering}
    \caption{Features in an image}
    \label{fig:f1}
\end{figure}

To understand neural networks, it is important to also understand how an image is represented in a computer. In the case of gray scale images, these are interpreted by a computer as an array. A grid of values for each grid cell is called a pixel, and each pixel has a numerical value. In the MNIST database, each image is 28 pixels height and wide. Therefore, these images are understood by a computer as a 28 by 28 array. In a typical gray scale image, white pixels are encoded as the value 255, and black pixels are encoded as zero. Gray pixels fall somewhere in between, with light-gray being closer to 255. This description can be easily undertood with Fig. \ref{fig:f2}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth,height=0.4\textheight,keepaspectratio]{images/image_representation.png}
    \captionsetup{justification=centering}
    \caption{Image representation as a matrix}
    \label{fig:f2}
\end{figure}

Nevertheless, the images in the MNIST dataset have actually gone through a quick pre-processing step. They've been re-scaled so that each image has pixel values in a range from zero to one Fig. \ref{fig:f3}. To go from a range of 0-255 to zero and one, it is necessary to divide every pixel value by 25 Eq. \eqref{eq:1}5. This step is called \textbf{normalization}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth,height=0.4\textheight,keepaspectratio]{images/image_norm.png}
    \captionsetup{justification=centering}
    \caption{Image representation as a normalized matrix}
    \label{fig:f3}
\end{figure}

\begin{equation}
NormX_{ij} = \frac{X_{ij}}{max(X)} \label{eq:1}
\end{equation}

Normalization is a useful tool to help the algorithm to train better. Thiss tep is quite useful due to neural networks rely on gradient calculations. These networks are trying to learn how important or how weighty a certain pixel should be in determining the class of an image. Normalizing the pixel values helps these gradient calculations stay consistent, and not get so large that they slow down or prevent a network from training.

Recall that to classify objects, the most common technique used for this was to use a MLP or fully connected neural network. However, these networks require 1-dimensional data. Hence, it is necessary to convert any array image into a vector. This process is called \textbf{flattening} and can be seen in Fig. \ref{fig:f4}. In this example, a four-by-four image which is a matrix with 16 pixel values. Instead of representing this as a four-by-four matrix, it is possible to construct a vector with 16 entries, where the first first entries of the vector correspond to the first row of the old array. The second four entries correspond to the second row and so on.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth,height=0.4\textheight,keepaspectratio]{images/flattening.png}
    \captionsetup{justification=centering}
    \caption{Flattening a simple matrix into a vector}
    \label{fig:f4}
\end{figure}

Once an image has been flattened, it is easy to pass the data to a MLP and obtain the classification result of the image.

It is important to note that Pytorch does Data normalization by subtracting the mean (the average of all pixel values) from each pixel, and then dividing the result by the standard deviation of all the pixel values.

\[\frac{P_{ij} - \mu}{\sigma}\]

Where for \(\mu = 0.5\) and \(\sigma=0.5\) the minimum (pixel coordinate equals 0) and maximum (pixel coordinate equals 1) values of the normalization are.

\[\frac{1 - 0.5}{0.5} = 1\]
\[\frac{0 - 0.5}{0.5} = -1\]

Resulting in a normalization of \([-1,1]\) for all the pixel values in the image.

A feasible network architecture to train the neural network is the one presented in Fig. \ref{fig:f50}. A MLP with 784 input units (number of inputs after flattening a MNIST image), two hidden layers with 512 nodes each and 10 output layers with a softmax activation function (one output per character) to produce a probability distribution in the final layer of the network.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\textwidth,height=0.55\textheight,keepaspectratio]{images/mlp.png}
    \captionsetup{justification=centering}
    \caption{MLP to train a MNIST image}
    \label{fig:f5}
\end{figure}

Recall that neural networks learn from mistakes, hence, the next list of stems gives a gives of how to optimize the weights of a neural network.

\begin{enumerate}
  \item Loss: Measure any mistake between a predicted and true class
  \item Backpropagation: Quantify how bad a particular weight is in making a mistake
  \item Optimization: Gives a way to calculate a better weight value
\end{enumerate}

As this problem is categorized as a multi-classes problem, the \textbf{loss function} needed is called \textbf{categorical cross-entropy loss}. This loss function is defined as the negative log of the output layer (with softmax activation function).

Therefore, if the probability of a node is \(p(x_i) = 0.162\) the negative log is given in Eq. \eqref{eq:2}.

\begin{equation}
loss = -\log(0.162) = 1.82 \label{eq:2}
\end{equation}

If the probability is something like \(p(x_i) = 0.441\), then the loss would be \(loss = 0.819\)

It is possible to conclude that the categorical cross-entropy loss is \textbf{lower} when the prediction and label agree and \textbf{higher} when the prediction and label disagree (a probability close to one yields to a low loss value).

The optimizer is used to minimize the error of the loss function and change the weights towards the minimum error expression. In the Fig. \ref{fig:f6}, in the top right box it is possible to see a list of different optimizer and their performance minimizing the error. Each optimizer has its own benefits and the selection of one or other depends on the problem.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth,height=0.4\textheight,keepaspectratio]{images/optimizer.png}
    \captionsetup{justification=centering}
    \caption{Optimizer used to train a neural network}
    \label{fig:f6}
\end{figure}

A complete solution to the problem described before can be seen in \textit{mnist\_mlp\_exercise.ipynb}.

To decide whether the model is doing well or bad during training, a new set needs to be introduced. \textbf{The validation set} is defined as a sub-sample of the training set, usually the 20\% of the training set. During training, the training set is used to change the weights and the validation set checks the performance of the model to avoid over-fitting. Finally, once a model has been chosen, a test set is used (this set has never been seen by the network before) to check the accuracy of the trained model. The pipeline described before can be seen in Fig. \ref{fig:f7}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\textwidth,height=0.55\textheight,keepaspectratio]{images/sets.png}
    \captionsetup{justification=centering}
    \caption{Sets the evaluate and train a model}
    \label{fig:f7}
\end{figure}

Overall, the steps to train a neural network are:

\begin{itemize}
  \item Visualize data
  \item Pre-process normalize transform
  \item Define a model
  \item Train the model define loss \& optimization function
  \item Save the best model
  \item Test the model
\end{itemize}

Despite that flattening images and using multi-layer perceptrons might sound like a good solution to deal with images, these techniques have a set of drawbacks which affects its implementation in real world images. For example, the MLP only uses fully-connected layers, which for the case of the MNIST dataset is almost half a million parameters. Furthermore, this sort of architecture only support data in the form on vectors whereas the image is a matrix with spatial information which can be relevant around its close neighborhood. 

To sort this problems a new architecture is employed and this is \textbf{called convolutional neural networks (CNNs)}. This networks instead of use fully connected layers use sparsely connected layers where the connection between layers are informed by the 2-D structure of the image matrix. Moreover, this network architecture accepts matrices as inputs. 

To understand the concept of sparsely connected and its benefits, consider breaking the image into four regions Fig. \ref{fig:f8}. The image is painted in four sections: red, green, yellow, and blue. Then, each hidden node could be connected to only the pixels in one of these four regions.Here, each headed node sees only a quarter of the original image. With this regional breakdown and the assignment of small local groups of pixels to different hidden nodes, every hidden node finds patterns in only one of the four regions in the image. Then, each hidden node still reports to the output layer where the output layer combines the findings for the discovered patterns learned separately in each region

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\textwidth,height=0.55\textheight,keepaspectratio]{images/sparsely.png}
    \captionsetup{justification=centering}
    \caption{Sets the evaluate and train a model}
    \label{fig:f8}
\end{figure}

This locally connected layer uses far fewer parameters than a densely connected layer. It's less prone to overfitting and truly understands how to tease out the patterns contained in image data. 

Rearranging the vector as a matrix it is possible to see the relationships between the nodes in each layer. It is possible to expand the number of patterns that the network is able to detect while still making use of the 2-D structure to selectively and conservatively add weights to the model by introducing more hidden nodes, where each is still confined to analyzing a single small region within the image. The red nodes in the hidden layer are still only connected to the red nodes in the input layer, with the same color coding for all other colors Fig. \ref{fig:f9}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\textwidth,height=0.55\textheight,keepaspectratio]{images/matrix_input.png}
    \captionsetup{justification=centering}
    \caption{Arranging the matrix as an input for a CNNs}
    \label{fig:f9}
\end{figure}

Recall that by expanding the number of nodes in the hidden layer it is possible to discover more complex patterns in the data. In this case, two collections of hidden nodes is being used where each collection contains nodes responsible for examining a different region of the image as shown by Fig. \ref{fig:f10}. It will prove useful to have each of the hidden nodes within a collection share a common group of weights. The idea being that different regions within the image can share the same kind of information. In other words, every pattern that's relevant towards understanding the image could appear anywhere within the image.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\textwidth,height=0.55\textheight,keepaspectratio]{images/shared_weights.png}
    \captionsetup{justification=centering}
    \caption{Sharing weights in a CNNs}
    \label{fig:f10}
\end{figure}

A convolutional neural network is a special kind of neural network which can remember spatial information. The common neural networks only look at individual inputs, whereas convolutional neural networks, can look at an image as a whole, or in patches and analyze groups of pixels at a time.

The key to preserving the \textbf{spatial information} in CNNs is called the convolutional layer. A \textbf{convolutional layer} applies a series of different image filters also known as \textbf{convolutional kernels} to an input image. The resulting filtered images have different appearances. The filters may have extracted features like the edges of objects in that image, or the colors that distinguish the different classes of images.

Spatial patterns in an image can be for example color or shape. The changes in shape can be described as the change of intensity in a group of pixels. These changes can be used to detect objects in an image. It is possible to identify these changes with convolutional kernels with a specific form and values.

To measure how the pixels change in an image, the frecuency can be used to identify the rate of change. A high frequency image Fig. \ref{fig:f11} is one where the intensity changes a lot  and the level of brightness changes quickly from one pixel to the next (pink square). A low frequency image may be one that is relatively uniform in brightness or changes very slowly (blue square). 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\textwidth,height=0.55\textheight,keepaspectratio]{images/frec.png}
    \captionsetup{justification=centering}
    \caption{High and low frecuency image patterns}
    \label{fig:f11}
\end{figure}

High-frequency components in an image also correspond to the edges of objects and can be used to classify them.

In an image, \textbf{filters} are used to filter out unwanted information or amplify features of interest as corners. An example of a filter is a high-pass filter which is used to sharpen an image and enhance high-frecuency parts of an image. In the Fig. \ref{fig:f12} the high-pass filter has enhanced the areas of the image where rapid changes have been presented (emphasizing edges). It is important to mention that this simple filters have to be used on one-channel images.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\textwidth,height=0.55\textheight,keepaspectratio]{images/high_pass.png}
    \captionsetup{justification=centering}
    \caption{Image after a High-pass filter}
    \label{fig:f12}
\end{figure}

A filter or convolutional filter is just a matrix of numbers that modifies the image. A high-ass filter is the one shown in Fig. \ref{fig:f13}. For edge detection, all the values inside the filter must add to zero.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.2\textwidth,height=0.2\textheight,keepaspectratio]{images/filter.png}
    \captionsetup{justification=centering}
    \caption{High-pass filter}
    \label{fig:f13}
\end{figure}

To apply a filter to an image an operation called convolution needs to be applied to the image. In this operation, the kernel sweeps the whole image, multiplying value by value the pixel and kernel. Finally, all the values are added and the result is places in a new image which will counting the result of the convolution. The image Fig. \ref{fig:f14} shows this convolution process in a simple set of pixels in an image. Is is important to mention that the convolution is denoted by the symbol \(\*\).

\[K * I(x,y) = \text{resulting image}\]

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\textwidth,height=0.55\textheight,keepaspectratio]{images/convolution.png}
    \captionsetup{justification=centering}
    \caption{The convolution process}
    \label{fig:f14}
\end{figure}

When applying this process to the corners of an image leads to unknown pixel values, techniques as extend, padding or crop are applied in the input image to generate an image of the same size of the input one. 

Getting more into detail, CNNs are a kind of deep learning model that can learn to do things like image classification and object recognition. They keep track of spatial information and learn to extract features like the edges of objects in something called a convolutional layer. The Fig. \ref{fig:f15} shows a simple CNN structure, made of multiple layers.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth,height=0.65\textheight,keepaspectratio]{images/layers_cnn.png}
    \captionsetup{justification=centering}
    \caption{Layers in a CNN}
    \label{fig:f15}
\end{figure}

The convolutional layer is produced by applying a series of many different image filters, also known as convolutional kernels, to an input image Fig. \ref{fig:f16}. In this case, four kernels mean four differently filtered output images.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth,height=0.65\textheight,keepaspectratio]{images/conv_layer.png}
    \captionsetup{justification=centering}
    \caption{Convolutional layer}
    \label{fig:f16}
\end{figure}

When these outputs are stacked it is possible to form a complete convolutional layer with a depth of four Fig. \ref{fig:f17}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth,height=0.65\textheight,keepaspectratio]{images/depth.png}
    \captionsetup{justification=centering}
    \caption{Depth in a convolutional layer}
    \label{fig:f17}
\end{figure}

The main difference between a filter and a convolutional kernel is that neural networks will actually learn the best filter weights as they train on a set of image data. 


\printbibliography

\end{document}

