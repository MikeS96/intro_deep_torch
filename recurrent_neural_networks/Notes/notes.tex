\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[backend=biber]{biblatex}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{dsfont}
\addbibresource{bib.bib}
\setlength{\parindent}{0em}
\bibliography{bib}
\setlength{\parskip}{6pt}
\usepackage[margin=1.0in]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{url}

\title{Intro to deep learning with PyTorch}
\author{Miguel A. Saavedra-Ruiz}
\date{May 2020}
\linespread{1.0}

\nocite{*}


\begin{document}

\maketitle

\section*{Recurrent neural networks}

To understand what recurrent neural networks (RNN) are and the difference between RNN and LSTM imagine the next example. Imagine a regular neural network which recognizes images and the image in Fig. \ref{fig:f1} is fitted in the network and the neural neural network guesses that the image is most likely a dog with a small chance of being a wolf and an even smaller chance of being a goldfish. However, what if the image is actually a wolf?


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.25\textwidth,height=0.25\textheight,keepaspectratio]{images/fitting.png}
    \captionsetup{justification=centering}
    \caption{Fitting an image in a neural network}
    \label{fig:f1}
\end{figure}

To hint the algorithm about what animal it really is, let's say that the wolf appeared in a TV show about nature and the previous image before the wolf was a bear and the previous one was a fox. In this case, the idea is to use this information to hint the algorithm that the last image is a wolf and not a dog. Therefore, a good idea is to analyze each image with the same copy of a neural network. But, using the output of the neural network as a part of the input of the next one Fig. \ref{fig:f2}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth,height=0.5\textheight,keepaspectratio]{images/recurrent.png}
    \captionsetup{justification=centering}
    \caption{Feeding a neural network with another neural networl}
    \label{fig:f2}
\end{figure}

This should improve the prediction results. Mathematically,it is just the combination of vectors in a linear function, which will then be squished with an activation function, which could be sigmoid or hyperbolic tan. This way the algorithm can use previous information and the final neural network will know that the show is about wild animals in the forest and actually use this information to correctly predict that the image is of a wolf and not a dog.

However, the last architecture has some drawbacks. Imagine that the bear appeared a while ago and the two recent images are a tree and a squirrel Fig. \ref{fig:f3}. Based on those two, it is hard to really know if the new image is a dog or a wolf. Since trees and squirrels are just as associated to domestic animals as they are with forest animals. The information about being in the forest comes all the way back from the bear. However, as it has been seen, information coming in gets repeatedly squished by sigmoid functions, training a network using backpropagation all the way back, will lead to problems such as the vanishing gradient problem. Hence by this point pretty much all the bear information has been lost. That's a problem with recurring neural networks; that the memory that is stored is normally short term memory.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth,height=0.65\textheight,keepaspectratio]{images/drawback_rnn.png}
    \captionsetup{justification=centering}
    \caption{Drawbacks with RNN}
    \label{fig:f3}
\end{figure}

To solve the last problem, LSTMs or long short term memory networks are the solution to this issue. 
In summary, a RNN works as follows; memory comes in and merges with a current event and the output comes out as a prediction of what the input is and also, as part of the input for the next iteration of the neural network Fig. \ref{fig:f4}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth,height=0.45\textheight,keepaspectratio]{images/rnn.png}
    \captionsetup{justification=centering}
    \caption{The general RNN architecture}
    \label{fig:f4}
\end{figure}

On the other hand, LSTM works as follows; it keeps track not just of memory but of long term memory, which comes in and comes out and also, short term memory, which also comes in and comes out Fig. \ref{fig:f5}. In every stage the long and short term memory in the event get merged. Therefore, a new long term memory, short term memory and a prediction are created. This architecture protects old information more.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\textwidth,height=0.55\textheight,keepaspectratio]{images/lstm.png}
    \captionsetup{justification=centering}
    \caption{The general LSTM architecture}
    \label{fig:f5}
\end{figure}

The LSTM architecture can be described using the following example. 

Imagine the next example Fig. \ref{fig:f6} where the long term memory is represented by an elephant, the short term memory by a fish and the event will still be represented by a wolf.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth,height=0.7\textheight,keepaspectratio]{images/lstm_example.png}
    \captionsetup{justification=centering}
    \caption{LSTM example}
    \label{fig:f6}
\end{figure}

So LSTM works as follows: the three pieces of information (Long term memory, short term memory and event Fig. \ref{fig:f6}) go inside the node some math happens inside which updates the output. These outputs are a long term memory, a short term memory and the prediction of the event. More specifically the architecture of the LSTM contains something called gates. It contains a forget gate, a learn gate, a remember gate, and a use gate Fig. \ref{fig:f7}.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth,height=0.7\textheight,keepaspectratio]{images/lstm_example2.png}
    \captionsetup{justification=centering}
    \caption{LSTM gates}
    \label{fig:f7}
\end{figure}

The detailed explanation of how these gates work is as follows. The long term memory goes to the forget gate where it forgets everything that it doesn't consider useful. The short term memory and the event are joined together in the learn gate, containing the information that the LSTM recently learned and it removes any unnecessary information.

Subsequently, the long term memory that have not been forgotten yet (Forget gate) plus the new information that the node learned (Learn gate) get joined together in the remember gate. Since it's called remember gate, what it does is it outputs an updated long term memory. An example of what a long term memory looks like is seen in Fig. \ref{fig:f7} (this information will be remembered in the future). Finally, the use gate is the one that decides what information will be used from what was previously know plus what the node just learned to make a prediction. Taking both inputs from the long term memory and the short term memory will create new information and hence, create a new short term memory output. The output becomes both the prediction and the new short term memory.
 
Overall, the information from one LSTM cell moves towards another one and another one saving information from the past and propagating it towards the future to produce accurate predictions and keep information from the past Fig. \ref{fig:f8}. The long term memory is denoted as \textbf{LTM} and the short term memory as \textbf{STM}. It is important to note that the short term memory is the output or the prediction of the network.
 
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth,height=0.7\textheight,keepaspectratio]{images/lstm_nodes.png}
    \captionsetup{justification=centering}
    \caption{LSTM nodes connection}
    \label{fig:f8}
\end{figure}
 
To continue with the explanation of the math behind LSTM, let's first recall the architecture of a RNN Fig. \ref{fig:f9}. Basically what this network does is take the event \(E_t\) and memory \(M_{t-1}\), coming from the previous point in time and apply a simple tanh or sigmoid activation function to obtain the output and then the memory \(M_t\). Specifically, the network joins these two vectors and multiply them by a matrix \(W\) and add a bias \(b\). Finally the output is squished with the tanh function, and that gives us the output \(M_t\) Eq. \eqref{eq:1}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth,height=0.5\textheight,keepaspectratio]{images/recurrent_node.png}
    \captionsetup{justification=centering}
    \caption{Mathematical operation in a recurrent node}
    \label{fig:f9}
\end{figure}


\begin{equation}
M_t = tanh(W[ST M_{t-1}, E_t] + b)
\label{eq:1}
\end{equation}

The LSTM architecture is very similar to the recurrent neural network's architecture, except with more nodes inside and with two inputs and outputs since it keeps track of the long- and short-term memories Fig. \ref{fig:f10}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth,height=0.65\textheight,keepaspectratio]{images/lstm_node.png}
    \captionsetup{justification=centering}
    \caption{Operations behind a LSTM node}
    \label{fig:f10}
\end{figure}

Let's start with the explanation of each gate in the LSTM. \textbf{The learn gate} by taking the short term memory and the event and it joins it. Additionally, it takes the short term memory and the event and it combines them and then it ignores a bit of it keeping the important part of it. Mathematically, it takes the short term memory \(STM_{t-1}\) and the event \(E_t\) and it combines them by putting them through a linear function which consists of joining the vectors multiplying by a matrix adding a bias and finally squishing the result with a tanh activation function Eq. \eqref{eq:4}.

\begin{equation}
N_t =  tanh(W_n[STM_{t-1}, E_t] + b_n)
\label{eq:5}
\end{equation}

To ignore information, an ignore factor \(i_t\) is introduces. This factor i actually a vector but it multiplies element wise. To calculate this vector previous information of the short term memory and the eventt are used. The Eq. \eqref{eq:6} describes the mathematical equation to compute this factor and the Fig. \ref{fig:f13} presents the learn gate pipeline.

\begin{equation}
i_t =  \sigma(W_i[STM_{t-1}, E_t] + b_i)
\label{eq:6}
\end{equation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth,height=0.65\textheight,keepaspectratio]{images/learn_gate.png}
    \captionsetup{justification=centering}
    \caption{The learn gate}
    \label{fig:f13}
\end{figure}



The \textbf{forget gate} takes a long term memory and it decides what parts to keep and to forget Fig. \ref{fig:f11}. Mathematically, The long-term memory (LTM) from \(t-1\) comes in and it gets multiplied by a Forget Factor \(ft\) Eq. \eqref{eq:2}. To calculate \(ft\) the short term memory STM and the event information are used as shown by Eq. \eqref{eq:3}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth,height=0.65\textheight,keepaspectratio]{images/forget_gate.png}
    \captionsetup{justification=centering}
    \caption{The forget gate}
    \label{fig:f11}
\end{figure}


\begin{equation}
LTM_{t-1} * ft
\label{eq:2}
\end{equation}


\begin{equation}
ft = \sigma(W_f[STM_{t-1}, E_t] + b_f)
\label{eq:3}
\end{equation}

\textbf{The remember gate} is the simplest. It takes the long-term memory coming out of the Forget Gate and the short-term memory coming out of the Learn Gate and simply combines them together Eq. \eqref{eq:4} (adding them) Fig. \ref{fig:f12}. The output of this gate is the new long term memory.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth,height=0.65\textheight,keepaspectratio]{images/remember_gate.png}
    \captionsetup{justification=centering}
    \caption{The remember gate}
    \label{fig:f12}
\end{figure}

\begin{equation}
LTM_t = LTM_{t-1} * ft + N_t * i_t
\label{eq:4}
\end{equation}

Finally, \textbf{the use gate} uses the long term memory that cames out of the forget gate and the short term memory that cames out of the learned gate, to come up with a new short term memory and an output. Mathematically, it applies

a small neural network on the output of the forget gate using the tanh activation function (Eq. \eqref{eq:7}) and it applies another small neural network on the short term memory and the events using the sigmoid activation function (Eq. \eqref{eq:8}). As a final step, it multiplies these two in order to get the new output Eq. \eqref{eq:9}. The output also worth of the new short term memory Fig. \ref{fig:f14}.

\begin{equation}
U_t = tanh(W_u LTM_{t-1} * f_t + b_u) 
\label{eq:7}
\end{equation}

\begin{equation}
V_t = \sigma(W_v[STM_{t-1}, E_t] + b_v)
\label{eq:8}
\end{equation}

\begin{equation}
STM_t = U_t * V_t
\label{eq:9}
\end{equation}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth,height=0.65\textheight,keepaspectratio]{images/use_gate.png}
    \captionsetup{justification=centering}
    \caption{The use gate}
    \label{fig:f14}
\end{figure}

Putting together all the gates will result again in the Fig. \ref{fig:f7}. As a reminder it works as follows: The forget gate takes the long-term memory and forgets part of it. The learn gate puts the short-term memory together with the event as the information recently learned. The remember gate joins the long-term memory that has not yet forgotten plus the new information learned in order to update the long-term memory and output it. Finally, the use gate also takes the information recently learned together with long-term memory that haven't yet been forgotten, and it uses it to make a prediction and update the short-term memory.

To introduce the Long term memory term in the forget factor for example, it is necessary to just connect the LTM into the neural network that calculates the forget factor.

Mathematically, this means that the input matrix is larger since the LTM is also concatenating with the long-term memory matrix Eq. \eqref{eq:10}. This is called a\textbf{ peephole connection} since now the long-term memory has more access into the decisions made inside the LSTM. This can be seen in Fig. \ref{fig:f15} where a peegole connection was add to the forget gate.

\begin{equation}
ft = \sigma(W_f[LTM_{t-1},STM_{t-1}, E_t] + b_f)
\label{eq:10}
\end{equation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth,height=0.65\textheight,keepaspectratio]{images/peehole.png}
    \captionsetup{justification=centering}
    \caption{Peehole connections}
    \label{fig:f15}
\end{figure}

Similarly, it is possible to do this to all the forget-type nodes in the LSTM and the result will be a LSTM with peephole connections Fig. \ref{fig:f16}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth,height=0.65\textheight,keepaspectratio]{images/peehole_lstm.png}
    \captionsetup{justification=centering}
    \caption{Peehole connections in a LSTM}
    \label{fig:f16}
\end{figure}

A simple implementation of a RNN to analize a time-serie of data can be seen in Simple\_RNN.ipynb.

Despite the simplicity of simple RNN and the lots of improvements provided by LSTM cells. There are other recurrent network's architectures that are quite powerful. One of these is \textbf{the Gate Recurrent Unit} (GRU). 

The GRU combines the forget and the learn gate into an update gate and then runs this through a combine gate. It only returns one working memory instead of a pair of long and short-term memories. Nevertheless, this network architecture works very well in practice Fig. \ref{fig:f17}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth,height=0.65\textheight,keepaspectratio]{images/gru.png}
    \captionsetup{justification=centering}
    \caption{The gate recurrent unit}
    \label{fig:f17}
\end{figure}

To explain better the general workflow of a recurrent neural network and its architecture, it is necessary to go back to the fundamental recurrent unit Fig. \ref{fig:f18}. The recurrent neural network is like a common neuron unit, except that it has feedback from its output. Unrolling the recurrent unit leads to the representation in the right of Fig. \ref{fig:f18}. The first input at \(t = t-1\) is the first input, then the output of this time is feed to the \textbf{same} recurrent unit at \(t = t\) and so on. The recurrent unit is just a simple unit, nevertheless, for better comprehension it is better to unroll it and see its behavior with the data in multiple time steps. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\textwidth,height=0.55\textheight,keepaspectratio]{images/unrolling.png}
    \captionsetup{justification=centering}
    \caption{Unrolling a recurrent neural network}
    \label{fig:f18}
\end{figure}

Furthermore, it is also possible to connect multiple recurrent units in a layer. This sort of configuration will lead to better interpretation of data in series. The Fig. \ref{fig:f19} shows the connection between one input unit and three hidden units. At the end, the output of the three recurrent units \(y\) is then feed into the three units again to generate the output at the next time step. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth,height=0.65\textheight,keepaspectratio]{images/mutiple_memorycells.png}
    \captionsetup{justification=centering}
    \caption{A layer with three recurrent units}
    \label{fig:f19}
\end{figure}

At this point, it is important to mention that a recurrent unit has two sets of weights, \(W_x, W_y\). \(W_x\) can be interpreted as the set of weights between the input units and the recurrent units. \(W_y\) is the set of weights between the output of the recurrent units and the hidden units. Therefore, in the first pass the equation of the output of the recurrent units in Fig. \ref{fig:f19} would be.

\[y_0 = \sigma(W_x * x + b)\]

Once the first time step has passed, the output with feedback at the second time step will be.

\[y_1 = \sigma(W_x * x + W_y * y_0 + b)\]

This process is repeated and repeated until the sequence of data is over. Unrolling a the recurrent neural network with three hidden unit in Fig. \ref{fig:f19} leads to the Fig. \ref{fig:f20} where is possible to see how the layer of hidden units is unrolled and its input is then used to feed the next time step.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth,height=0.5\textheight,keepaspectratio]{images/unrolling_multiple.png}
    \captionsetup{justification=centering}
    \caption{Unrolling a recurrent neural network with three recurrent units}
    \label{fig:f20}
\end{figure}

There are multiple configurations and uses for this network architectures. The first configuration is called sequence to sequence and this configuration is useful to create a sequence as output from a sequence of data as input Fig. \ref{fig:f21}. An example would be a time series of data of houses sales and the output would be a sequence of sales shifted one time step in the future.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth,height=0.45\textheight,keepaspectratio]{images/sequence_sequence.png}
    \captionsetup{justification=centering}
    \caption{Sequence to sequence}
    \label{fig:f21}
\end{figure}

Another configuration is called sequence to vector and is used to generate a vector out of a sequence of data Fig. \ref{fig:f22}. An example would be sentiments scores, feed a sequence of words (paragraph) and then request a vector indicating whether it was a positive sentiment or a negative sentiment (1 or 0).

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth,height=0.45\textheight,keepaspectratio]{images/sequence_vector.png}
    \captionsetup{justification=centering}
    \caption{Sequence to vector}
    \label{fig:f22}
\end{figure}

Finally, the last configuration is called vector to sequence Fig. \ref{fig:f23}. This sort of architecture is used to predict a sequence of data out of a simple vector. An example of this is pass a single image and then request a sequence of words describing the content of the image. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth,height=0.45\textheight,keepaspectratio]{images/vector_sequence.png}
    \captionsetup{justification=centering}
    \caption{Vector to sequence}
    \label{fig:f23}
\end{figure}

To correctly feed a sequence of words, audio or data in a recurrent neural network, it is necessary to understand the concept of sequence batching. To understand this concept, lets use an example Fig. \ref{fig:f24}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth,height=0.45\textheight,keepaspectratio]{images/sequence_batching.png}
    \captionsetup{justification=centering}
    \caption{Sequence batching}
    \label{fig:f24}
\end{figure}

Imagine a sequence of numbers from 1 to 12. This sequence can be passed into a RNN as one sequence. However, it is possible to split it in half and pass in two sequences. The batch size corresponds to the number of sequences now used hence, the batch size is 2. Along with the batch size it is also possible to choose the length of the sequences that will be feed to the network. For example, let's consider using a sequence length three. Then the first batch of data that will pass into the network are the first 3 values in each mini sequence. The next batch contains the next three values and so on until the data is over Fig. \ref{fig:f24}. It is possible to retain the hidden state from one batch and use it at the start of the next batch. This way the sequence information is transferred across batches for each mini sequence.

A complete exercise with LSTM implemented in IPython for text analysis can be seen at \textit{Character\_Level\_RNN.ipynb}.

One application of RNN is \textbf{Sentiment Analysis and Prediction}. The idea is to build a model that can read some text and make a prediction about the sentiment of that text, whether it is positive or negative. It is possible to train such model on a dataset of movie reviews from IMDB that have been labeled either "positive" or "negative". Since this is text data, words in a sequence, this problem is ideal to a RNN which doesn't only consider the individual words, but the order they appear in. This leads to a powerful model for making these types of sentiment predictions. This model is an example of sequence to vector where only the last output will be used.

The implementation of the last mentioned can be found at \textit{Sentiment\_RNN.ipynb}. This implementation deals with the pre-processing of text data, the embedding process, model definition, training and validation. Additionally, only the output of the last sequence is taken to give the final result about the analysis of the movie. 




\printbibliography


\end{document}

